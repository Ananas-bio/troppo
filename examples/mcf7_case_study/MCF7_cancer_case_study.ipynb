{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649a2f1a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# A pipeline for large-scale reconstruction and validation of context-specific models\n",
    "\n",
    "This case study examines the pipeline used by [Vieira, Ferreira, and Rocha (2022)](https://doi.org/10.1371/journal.pcbi.1009294) for large-scale context-specific model reconstruction. The authors employed this approach to reconstruct over 6,000 models for 733 cell lines from the Cancer Cell Line Encyclopedia (CCLE) database. Their methodology was validated using an MCF7 cell line sample, generating 320 models through various algorithms and parameter combinations. The best parameter combinations were then selected for subsequent reconstructions.\n",
    "\n",
    "In this notebook, the focus will be the reconstruction of context-specific models for the MCF7 cell line using the [Human-GEM](https://github.com/SysBioChalmers/Human-GEM/tree/main) model as a template. The dataset was obtained from all 733 CCLE samples to calculate transcript activity scores (TASs) for each of the 80 possible combinations of the Global (5 combinations), Local T1 (25 combinations), and Local T2 (50 combinations) strategies. From these, we selected the combinations specific to the MCF7 cell line (ACH-000019). Each combination was reconstructed using either a min/max or min/sum function to calculate scores for each reaction with FastCORE or tINIT, resulting in 320 context-specific models. These models were further refined using EFM gap-fill and validated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d1b91",
   "metadata": {},
   "source": [
    "### Pipeline Overview\n",
    "\n",
    "The pipeline employed for this case study will follow these steps:\n",
    "\n",
    "1. **Context-Specific Model Reconstruction**: Reconstruct 320 context-specific models for the MCF7 cell line using all possible scoring combinations with both FastCORE and tINIT algorithms.\n",
    "2. **Fine-Tune Models with Gap-Filling**: Apply gap-filling for growth under open medium conditions using the `EFMGapfill` method.\n",
    "3. **Task Evaluation**: Evaluate the performance of metabolic tasks for the fine-tuned models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12eaa74",
   "metadata": {},
   "source": [
    "### 1. Contex-Specific Model Reconstruction\n",
    "\n",
    "##### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97100649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:17:00.456722Z",
     "start_time": "2024-08-06T17:16:56.759996Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from cobra.io import read_sbml_model\n",
    "from cobamp.utilities.parallel import batch_run\n",
    "from troppo.methods_wrappers import ReconstructionWrapper\n",
    "from troppo.omics.core import TypedOmicsMeasurementSet, IdentifierMapping\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf654ed9",
   "metadata": {},
   "source": [
    "##### Define Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2c61e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:17:00.471749Z",
     "start_time": "2024-08-06T17:17:00.458723Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/CCLE_expression_ACH-000019_scores.csv'\n",
    "CS_MODEL_DF_FOLDER = 'results/reconstructions/mcf7_comparison'\n",
    "CS_MODEL_NAMES = 'cs_models_all_combinations'\n",
    "GENE_NAME_GRAB_PATTERN = 'ENSG[0-9]*'\n",
    "PROTECTED_REACTION_LIST = 'biomass_human,HMR_10023,HMR_10024'\n",
    "ALGORITHM_OPTIONS = 'fastcore,tinit'\n",
    "INTFUNCTION_OPTIONS = 'minsum,minmax'\n",
    "INDEX_COLUMNS = '0,1,2,3'\n",
    "NTHREADS = 12\n",
    "CS_MODEL_DF_FILE = os.path.join(CS_MODEL_DF_FOLDER, CS_MODEL_NAMES + '.csv')\n",
    "\n",
    "if not os.path.exists(CS_MODEL_DF_FOLDER):\n",
    "    os.makedirs(CS_MODEL_DF_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ef52c",
   "metadata": {},
   "source": [
    "##### Read and Process the Omics Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38c5e0",
   "metadata": {},
   "source": [
    "For this case study the input will be the TASs for the ACH-000019 sample of the CCLE dataset. TAS were preivously calculated using the `GeneLevelThresholding` class for all possible combinations of thresholding strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3beef96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:28:59.460360Z",
     "start_time": "2024-08-06T14:28:51.799278Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is needed only if you need to convert genes from one nomenclature to another\n",
    "mapping = IdentifierMapping('human_transcriptomics',\n",
    "                            pd.read_csv('ftp://ftp.ebi.ac.uk/pub/databases/genenames/hgnc/tsv/non_alt_loci_set.txt',\n",
    "                                        index_col=0, sep='\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5afd21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:29:10.745867Z",
     "start_time": "2024-08-06T14:28:59.463884Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_data = pd.read_csv(DATA_PATH, index_col=list(map(int, INDEX_COLUMNS.split(','))))\n",
    "exp_data = exp_data.tail(2)\n",
    "exp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3263867",
   "metadata": {},
   "source": [
    "Load the dataset into a `OmicsMeasurmentSet` object with the dataframe components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a2952",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:29:10.761952Z",
     "start_time": "2024-08-06T14:29:10.748866Z"
    }
   },
   "outputs": [],
   "source": [
    "omics_mset = TypedOmicsMeasurementSet(exp_data.index, exp_data.columns, exp_data.values, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4db22e",
   "metadata": {},
   "source": [
    "The gene identifiers on the dataset have two different nomenclatures, HGNC Symbol and Ensembl Gene ID. Since the model that is going to be uses the Ensembl Gene IDs, the column names must be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354fae79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:29:10.837263Z",
     "start_time": "2024-08-06T14:29:10.764948Z"
    }
   },
   "outputs": [],
   "source": [
    "if GENE_NAME_GRAB_PATTERN != '':\n",
    "    ensembl_patt = re.compile(GENE_NAME_GRAB_PATTERN)\n",
    "    omics_mset.column_names = [ensembl_patt.findall(k)[0] for k in omics_mset.column_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd443836",
   "metadata": {},
   "source": [
    "After converting the gene IDs to the same nomenclature as the genes in the model, the data will be converted to a dictionary to simplify the process of generating all possible combinations to be used for each algorighm on the reconstruction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead2e9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:29:10.992104Z",
     "start_time": "2024-08-06T14:29:10.839265Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dicts = {'_'.join(map(str, k)): v for k, v in omics_mset.data.T.to_dict().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f6b85e",
   "metadata": {},
   "source": [
    "The biomass reaction will also be marked as a protected reaction so it doesn't get removed from the core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410002e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:29:11.007105Z",
     "start_time": "2024-08-06T14:29:10.994074Z"
    }
   },
   "outputs": [],
   "source": [
    "protected = list(map(lambda x: x.strip(), PROTECTED_REACTION_LIST.split(',')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49ba82",
   "metadata": {},
   "source": [
    "##### Generate all possible combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216bb11",
   "metadata": {},
   "source": [
    "To use all possible combinations, each of the thresholding strategies need to be tested using different and/or functions during the `ScoreIntegration` process.\n",
    "These are associated with how the score is atributed to reaction according to its Gene-Protein rule (GPRs), more specificaly:\n",
    "\n",
    "- `and_func`: a function that is used to combine the scores of the genes associated with a reaction for AND rules in the GPR. Tipically, the minimum function is used. This means that the score of a reaction with AND in their GPRs will be the minimum score of the genes associated with it.\n",
    "- `or_func`: a function that is used to combine the scores of the genes associated with a reaction for OR rules in the GPR. Tipically, sum or maximum function can be used. If the `sum` function is used, the score of a reaction with OR in their GPRs will be the sum of the scores of the genes associated with it. If is `max` used score for the gene with the maximum score will be used.\n",
    "\n",
    "Hence, the following step is to generate a dictionary with all combinations of parameters plus and/or functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avaliable_funcs = list(map(lambda x: x.strip(), INTFUNCTION_OPTIONS.split(',')))\n",
    "funcs = {k: v for k, v in {'minsum': (min, sum),\n",
    "                           'minmax': (min, max)}.items() if k in avaliable_funcs}\n",
    "runs = dict(chain(*[[((k, n), (aof, v)) for k, v in data_dicts.items()] for n, aof in funcs.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b91920",
   "metadata": {},
   "source": [
    "##### Load the Generic HumanGEM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515a9e4",
   "metadata": {},
   "source": [
    "For this case study a simplified version the generic Human-GEM model was used. The changes performed to the model were the following:\n",
    "\n",
    "1. Remove Boundary Metabolites: Eliminate boundary metabolites from the model.\n",
    "2. Remove Blocked Reactions: Exclude reactions that are blocked and do not contribute to the metabolic network.\n",
    "3. Adjust Lower Bound of `HMR_10024` Reaction: Set the lower bound of the `HMR_10024` reaction to zero. This reaction represents an exchange reaction for biomass and is necessary to close it to ensure that biomass is not entering the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6512bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:30:14.393975Z",
     "start_time": "2024-08-06T14:29:34.594698Z"
    }
   },
   "outputs": [],
   "source": [
    "model = read_sbml_model('data/Human-GEM.xml')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e1f2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:47:47.648704Z",
     "start_time": "2024-08-06T14:47:47.623625Z"
    }
   },
   "outputs": [],
   "source": [
    "model.reactions.get_by_id('HMR_10024').bounds = (0, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36005f21",
   "metadata": {},
   "source": [
    "#### Create a model wrapper.\n",
    "\n",
    "The `ModelBasedWrapper` class is used to wrap the model so that it can be used by *Troppo*.\n",
    "\n",
    "Relevant arguments from this class include:\n",
    "- `model`: the model to be wrapped.\n",
    "- `ttg_ratio`: the ratio between the number of reactions to be selected and the total number of reactions in the model.\n",
    "- `gpr_gene_parse_function`: a function that parses the GPRs of the model. This is used to map the identifiers in the omics data to the identifiers in the model.\n",
    "\n",
    "Important attributes from this class include:\n",
    "- `model_reader`: a COBRAModelObjectReader instance containing all the information of the model, such as, reaction_ids, metabolite_ids, GPRs, bounds, etc.\n",
    "- `S`: the stoichiometric matrix of the model.\n",
    "- `lb`: the lower bounds of the reactions in the model.\n",
    "- `ub`: the upper bounds of the reactions in the model.\n",
    "\n",
    "In this specific example we will use the `ReconstructionWrapper` class instead of the base `ModelBasedWrapper` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa085e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:48:06.364920Z",
     "start_time": "2024-08-06T14:47:52.521615Z"
    }
   },
   "outputs": [],
   "source": [
    "rw = ReconstructionWrapper(model, ttg_ratio=9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41cf6a8",
   "metadata": {},
   "source": [
    "##### Model Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5128729",
   "metadata": {},
   "source": [
    "The reconstruction process will be carried out through batch functions for a more time-efficient process. To achieve this, it is first necessary to define the functions that will be parallelized. These functions must be specific to the reconstruction algorithm being used and follow a specific structure:\n",
    "\n",
    "1. Convert the omics data into an `OmicsContainer` object.\n",
    "2. Define a custom `ScoreIntegration` function.\n",
    "3. Apply `run_from_omics` method from the `ReconstructionWrapper` object.\n",
    "\n",
    "Parameters required for the `run_from_omics` function:\n",
    "\n",
    "- `omics_data`: The omics data container for the sample.\n",
    "- `algorithm`: A string specifying the reconstruction algorithm.\n",
    "- `and_or_funcs`: A tuple containing functions for the AND and OR operations of the GPR.\n",
    "- `integration_strategy`: A tuple with the integration strategy and the function to apply to the scores.\n",
    "- `solver`: The solver for optimization.\n",
    "- `**kwargs`: Additional parameters specific to the algorithm used.\n",
    "\n",
    "*Output:*\n",
    "\n",
    "The `run_from_omics` method returns a dictionary with all model reactions and their respective Boolean values, indicating whether each reaction is active in the model. Note that this function will run for each sample individually, so the dictionary will only contain results for one sample at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd20b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:48:27.410815Z",
     "start_time": "2024-08-06T14:48:27.397785Z"
    }
   },
   "outputs": [],
   "source": [
    "def fastcore_reconstruction_func(score_tuple, params):\n",
    "    from troppo.omics.core import OmicsContainer\n",
    "    aofx, data_dict = score_tuple\n",
    "    oc_sample = OmicsContainer(omicstype='transcriptomics', condition='x', data=data_dict, nomenclature='custom')\n",
    "    r_wrapper = [params[k] for k in ['rw']][0]  # load parameters\n",
    "    protected_core = params['protected']\n",
    "    t = 0\n",
    "\n",
    "    def integration_fx(data_map):\n",
    "        return [[k for k, v in data_map.get_scores().items() if\n",
    "                 (v is not None and v > t) or k in protected_core]]\n",
    "\n",
    "    try:\n",
    "        return r_wrapper.run_from_omics(omics_data=oc_sample, algorithm='fastcore', and_or_funcs=aofx,\n",
    "                                        integration_strategy=('custom', [integration_fx]), solver='CPLEX')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {r: False for r in r_wrapper.model_reader.r_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796203c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:48:28.591933Z",
     "start_time": "2024-08-06T14:48:28.569468Z"
    }
   },
   "outputs": [],
   "source": [
    "def tinit_reconstruction_func(score_tuple, params):\n",
    "    from troppo.omics.core import OmicsContainer\n",
    "    aofx, data_dict = score_tuple\n",
    "    oc_sample = OmicsContainer(omicstype='transcriptomics', condition='x', data=data_dict, nomenclature='custom')\n",
    "    r_wrapper = [params[k] for k in ['rw']][0]  # load parameters\n",
    "    protected_core = params['protected']\n",
    "    try:\n",
    "        def tinit_integration_fx(data_map):\n",
    "            maxv = max([k for k in data_map.get_scores().values() if k is not None])\n",
    "            scores = {k: (v / maxv if v < 0 else v) if v is not None else 0 for k, v in data_map.get_scores().items()}\n",
    "            scores.update({x: max(scores.values()) for x in protected_core})\n",
    "            return scores\n",
    "\n",
    "        return r_wrapper.run_from_omics(omics_data=oc_sample, algorithm='tinit', and_or_funcs=aofx,\n",
    "                                        integration_strategy=('custom', [tinit_integration_fx]), solver='CPLEX')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {r: False for r in r_wrapper.model_reader.r_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6265cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:48:29.651954Z",
     "start_time": "2024-08-06T14:48:29.640265Z"
    }
   },
   "outputs": [],
   "source": [
    "# func_map = {'tinit': tinit_reconstruction_func, 'fastcore': fastcore_reconstruction_func}\n",
    "func_map = {'tinit': tinit_reconstruction_func}\n",
    "safe_threads = {'tinit': 1, 'fastcore': 2}\n",
    "result_dicts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118babb6",
   "metadata": {},
   "source": [
    "Define the `bacth_reconstruct` function to parse the configurations and run the `batch_run` method from COBAMP. \n",
    "For this function, the required parameters are:\n",
    "\n",
    "- `function`: The function to be parallelized.\n",
    "- `sequence`: The sequence where the parallelized function is to be applied.\n",
    "- `paramargs`: The specific parameters of the function.\n",
    "- `threads`: The number of threads to use for parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b33ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:48:31.142078Z",
     "start_time": "2024-08-06T14:48:31.133910Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_reconstruct(runs_config, rec_wrapper, core, rec_func, algo_name, max_threads=NTHREADS):\n",
    "    labs, iters = zip(*runs_config.items())\n",
    "    tlabs = [tuple([algo_name] + list(lab)) for lab in labs]\n",
    "    output = batch_run(rec_func, iters, {'rw': rec_wrapper, 'protected': core}, threads=min(len(runs_config), max_threads))\n",
    "    return dict(zip(tlabs, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81071aa",
   "metadata": {},
   "source": [
    "The final step is to call the `batch_reconstruct` function for each algorithm and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191fc5db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T14:55:48.625343Z",
     "start_time": "2024-08-06T14:55:48.579745Z"
    }
   },
   "outputs": [],
   "source": [
    "avaliable_algos = list(map(lambda x: x.strip(), ALGORITHM_OPTIONS.split(',')))\n",
    "for algname, func in func_map.items():\n",
    "    if algname in avaliable_algos:\n",
    "        result_dicts.update(batch_reconstruct(runs, rw, protected, func, algname, safe_threads[algname]))\n",
    "# pd.DataFrame.from_dict(result_dicts, orient='index').to_csv(CS_MODEL_DF_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(list(result_dicts.values())[1].values()).count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(list(result_dicts.values())[0].values()).count(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101cc65",
   "metadata": {},
   "source": [
    "For time saving, the results are available on the resuls folder. That `.csv` file can be loaded and used for the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv('results/reconstructions/mcf7_comparison/cs_models_all_combinations.csv', index_col=[0, 1, 2])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ea06d",
   "metadata": {},
   "source": [
    "### 2. Gap-fill for growth under open medium conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec79884",
   "metadata": {},
   "source": [
    "##### Imports and path definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobamp.wrappers.external_wrappers import get_model_reader\n",
    "from troppo.methods_wrappers import GapfillWrapper\n",
    "from troppo.methods.gapfill.efm import EFMGapfillProperties\n",
    "\n",
    "MODEL_DF_PATH_BIOMASS_GAPFILL = os.path.join(CS_MODEL_DF_FOLDER, CS_MODEL_NAMES + '_biomass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = read_sbml_model('data/Human-GEM.xml')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fad74b",
   "metadata": {},
   "source": [
    "For this case study, gap-filling was performed to ensure that biomass production was achieved for all models under open medium conditions. The first step in this process was to create a `ConstraintBasedModel` object from CoBAMP using the `to_cobamp_cbm()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cobamp_model = get_model_reader(model).to_cobamp_cbm('CPLEX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc754e8",
   "metadata": {},
   "source": [
    "To use a COBRA model object for gap-filling with TROPPO, a `ModelBasedWrapper` object is required. For this specific task, we will use the `GapfillWrapper` class instead of the base `ReconstructionWrapper` class used in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GapfillWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4455eab",
   "metadata": {},
   "source": [
    "Next all boundary reactions are retrieved to ensure they will be part of the final reactions of the fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8fe39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchanges = set(chain(*cobamp_model.get_boundary_reactions().values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67daef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gapfill_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80224e",
   "metadata": {},
   "source": [
    "For gap-filling, the following methods are required for each model:\n",
    "\n",
    "1. **Simulate the Reconstructed Model**: Check if biomass production is achieved. If not, perform gap-filling.\n",
    "2. **Run the EFM Algorithm**: Use the EFM algorithm to perform gap-filling.\n",
    "3. **Update Reconstruction Results**: Incorporate the reactions required to achieve biomass production into the set of reconstruction results.\n",
    "\n",
    "This method can be further adapted to ensure biomass production under specific medium conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in result_df.T.items():\n",
    "    if i not in gapfill_results.keys():\n",
    "        activity = row.to_dict()\n",
    "        \n",
    "        for k in exchanges:\n",
    "            activity[k] = True\n",
    "            \n",
    "        inactive_rxs = set([k for k, v in activity.items() if not v])\n",
    "        \n",
    "        with cobamp_model as m:\n",
    "            for rx in inactive_rxs:\n",
    "                m.set_reaction_bounds(rx, lb=0, ub=0)\n",
    "            sol = m.optimize({'biomass_human': 1}, False)\n",
    "            \n",
    "        if sol.status() != 'optimal' or sol.objective_value() < 1e-3:\n",
    "            print('Model', i, 'did not pass:', sol)\n",
    "            gapfill_sol = gw.run(avbl_fluxes=list(inactive_rxs),\n",
    "                                 algorithm='efm',\n",
    "                                 ls_override={'produced': ['temp001c']})\n",
    "            for k in gapfill_sol[0]:\n",
    "                activity[k] = True\n",
    "            with cobamp_model as m:\n",
    "                for rx in [k for k, v in activity.items() if not v]:\n",
    "                    m.set_reaction_bounds(rx, lb=0, ub=0)\n",
    "                gsol = m.optimize({'biomass_human': 1}, False)\n",
    "                print('Solution after gapfilling:', gsol)\n",
    "        else:\n",
    "            print('Model', i, 'passed:', sol)\n",
    "        gapfill_results[i] = activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b756e0b",
   "metadata": {},
   "source": [
    "Save the gap-fill results in a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7576448",
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_gapfill_models = pd.DataFrame(gapfill_results).T\n",
    "biomass_gapfill_models.to_csv(MODEL_DF_PATH_BIOMASS_GAPFILL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752014e",
   "metadata": {},
   "source": [
    "For time saving, the results are available on the resuls folder. That `.csv` file can be loaded and used for the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d894cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_gapfill_models = pd.read_csv(MODEL_DF_PATH_BIOMASS_GAPFILL, index_col=[0,1,2])\n",
    "biomass_gapfill_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad8f60",
   "metadata": {},
   "source": [
    "### 3. Evaluate Essential Tasks\n",
    "\n",
    "##### Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from troppo.tasks.core import TaskEvaluator\n",
    "from troppo.tasks.task_io import ExcelTaskIO\n",
    "from cobamp.utilities.file_io import pickle_object\n",
    "import numpy as np\n",
    "\n",
    "TASK_FILE_PATH = 'data/metabolicTasks_Essential.xlsx'\n",
    "CS_MODEL_SET_NAME = 'cs_models_all_combinations_biomass'\n",
    "TASK_RESULTS_FOLDER = os.path.join(CS_MODEL_DF_FOLDER, 'task_evaluation')   \n",
    "TASK_RESULTS_PATH = os.path.join(TASK_RESULTS_FOLDER, CS_MODEL_SET_NAME+'_taskeval.pkl')\n",
    "TASK_RESULTS_DF = os.path.join(TASK_RESULTS_FOLDER, CS_MODEL_SET_NAME+'_taskeval.csv')\n",
    "\n",
    "if not os.path.exists(TASK_RESULTS_FOLDER): os.makedirs(TASK_RESULTS_FOLDER)\n",
    "\n",
    "AVAILABLE_THREADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc96d44",
   "metadata": {},
   "source": [
    "The first step of the Task evaluation portion of the pipeline is to load a file with the metabolic tasks for the Human-GEM model. This file can be found on TROPPO's github page and will be loaded with the `ExcelTaskIO` subclass.\n",
    "\n",
    "Since the tasks file is a `.xlsx` file it is possible that a xlrd dependency is required. If it is the case, use pip to install the dependency into your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = ExcelTaskIO().read_task(TASK_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665bfa9",
   "metadata": {},
   "source": [
    "Further processing of the task list to account for changes made to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540da99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metab_map = {m.name + '[' + m.compartment + ']': m.id if m.compartment != 'x' else m.id[:-1] + 's' for m in\n",
    "                 model.metabolites}\n",
    "metab_map['NEFA blood pool in[x]'] = 'm02560s'\n",
    "replace_func = lambda x: metab_map[x] if x in metab_map.keys() else x\n",
    "\n",
    "for t in task_list: t.id_replace(replace_func)  # only works for inflow_dict/outflow_dict\n",
    "for t in task_list:\n",
    "    t.reaction_dict = {k: [{metab_map[i]: j for i, j in v[0].items() if i in metab_map.keys()}, v[1]]\n",
    "                       for k, v in t.reaction_dict.items()}\n",
    "\n",
    "task_list[27].outflow_dict.update(task_list[27].inflow_dict)\n",
    "del task_list[27].outflow_dict['ALLMETSIN[s]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b48474",
   "metadata": {},
   "source": [
    "Create a copy of the model to avoid making changes to the template model. Subsequently, boundary reactions are to be removed and drains set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb616fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_model = model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rx in task_model.boundary: rx.bounds = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4bc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_model_no_boundary = task_model.copy()\n",
    "task_model_no_boundary.remove_reactions(task_model_no_boundary.boundary)\n",
    "task_model_no_boundary.remove_reactions([k for k in task_model_no_boundary.reactions if len(k.metabolites) == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf776df",
   "metadata": {},
   "source": [
    "The class responsible for task evaluation in TROPPO is called `TaskEvaluator`. This class serves as a wrapper around a model, allowing for the evaluation of specific tasks on the model. It can be used to evaluate either a single task or a batch of tasks on a batch of models.\n",
    "\n",
    "To initialize a `TaskEvaluator` object, you need to pass a model object and the tasks to evaluate. The tasks should be instances of the `Task` class, which is a simple data structure containing the following fields:\n",
    "\n",
    "- `reaction_dict`: A dictionary with reaction identifiers as keys and a dictionary with metabolites and their respective stoichiometry as values. For example:\n",
    "  `rxd = {'r1':({'m1': -1, 'm2': 2}, (lb, ub)), ... }`\n",
    "\n",
    "- `inflow_dict`: A dictionary with metabolite identifiers as keys and the inflow rate as values. For example:\n",
    "  `inflow = {'m1': (1, 1), ... }`\n",
    "\n",
    "- `outflow_dict`: A dictionary with metabolite identifiers as keys and the outflow rate as values. For example:\n",
    "  `outflow = {'m5': (5, 5), ... }`\n",
    "\n",
    "This structure allows you to define and evaluate the metabolic tasks on your model efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085cf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tev = TaskEvaluator(model=task_model_no_boundary, tasks=task_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d40f4",
   "metadata": {},
   "source": [
    "The following lines represent the final step in the pipeline, where task evaluation is performed:\n",
    "\n",
    "1. **Split Data and Iterate**: The script splits the `biomass_gapfill_models` DataFrame into chunks based on the number of available threads (`AVAILABLE_THREADS`). It then iterates over these chunks.\n",
    "\n",
    "2. **Prepare Data for Evaluation**: Within each chunk, it prepares a list of boundary changes. Each change sets the lower and upper bounds of certain reactions to `(0, 0)` if the reaction value is not present in the data.\n",
    "\n",
    "3. **Evaluate Tasks**: The script evaluates the tasks in parallel using the `tev.batch_evaluate` method.\n",
    "\n",
    "4. **Update Results**: The evaluation results are updated in the `task_eval_results` dictionary. This dictionary is indexed by the original DataFrame indices, with each entry containing the evaluation results.\n",
    "\n",
    "This step ensures that all evaluated tasks are properly stored and managed after processing, completing the pipeline workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_eval_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sub_model_df in enumerate(np.array_split(biomass_gapfill_models[[r.id for r in task_model_no_boundary.reactions]], \n",
    "                                                AVAILABLE_THREADS)):\n",
    "    print('Model chunk #'+str(i+1))\n",
    "    bound_changes = [{k:(0, 0) for k,v in row.to_dict().items() if not v}\n",
    "                     for i, row in sub_model_df.iterrows()]\n",
    "    task_eval_results.update({sub_model_df.index[k]: {i:j[0] for i,j in v.items()}\n",
    "                              for k,v in tev.batch_evaluate(bound_changes, AVAILABLE_THREADS).items()})\n",
    "    pickle_object(task_eval_results,TASK_RESULTS_PATH+'.'+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf64364",
   "metadata": {},
   "source": [
    "The results will be saved in a pickle file. Nevertheless, we can convert the resulting dictionary into a pandas DataFrame to assess the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d8359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tevr = pd.DataFrame(task_eval_results).T\n",
    "tevr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ba257",
   "metadata": {},
   "outputs": [],
   "source": [
    "tevr.to_csv(TASK_RESULTS_DF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
